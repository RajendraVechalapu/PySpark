{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab1cf80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'install_exception_handler' from 'pyspark.sql.utils' (d:\\Rajendra-Tech\\GitHubRepos\\PySpark\\PySpark\\.venv\\Lib\\site-packages\\pyspark\\sql\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mPYSPARK_PYTHON\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mRajendra-Tech\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mGitHubRepos\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mPySpark\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mPySpark\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.venv\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mScripts\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mpython.exe\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m findspark.init()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession, Row\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdelta\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeltaTable\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# ✅ Create Spark session with Delta Lake support\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\spark\\spark-3.3.2-bin-hadoop3\\python\\pyspark\\__init__.py:144\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# for back compatibility\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SQLContext, HiveContext, Row  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m    146\u001b[39m __all__ = [\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSparkConf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSparkContext\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    168\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    169\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\spark\\spark-3.3.2-bin-hadoop3\\python\\pyspark\\sql\\__init__.py:43\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03mImportant classes of Spark SQL and DataFrames:\u001b[39;00m\n\u001b[32m     20\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m \u001b[33;03m      For working with window functions.\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Row\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SQLContext, HiveContext, UDFRegistration\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msession\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolumn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Column\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\spark\\spark-3.3.2-bin-hadoop3\\python\\pyspark\\sql\\context.py:39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m since, _NoValue\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_globals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _NoValueType\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msession\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _monkey_patch_RDD, SparkSession\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreadwriter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataFrameReader\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\spark\\spark-3.3.2-bin-hadoop3\\python\\pyspark\\sql\\session.py:60\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstreaming\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataStreamReader\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     50\u001b[39m     AtomicType,\n\u001b[32m     51\u001b[39m     DataType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     _parse_datatype_string,\n\u001b[32m     59\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m install_exception_handler, is_timestamp_ntz_preferred\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AtomicValue, RowLike\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'install_exception_handler' from 'pyspark.sql.utils' (d:\\Rajendra-Tech\\GitHubRepos\\PySpark\\PySpark\\.venv\\Lib\\site-packages\\pyspark\\sql\\utils.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "# ✅ Set Spark and Python environment\n",
    "os.environ[\"SPARK_HOME\"] = r\"D:\\spark\\spark-3.3.2-bin-hadoop3\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = r\"D:\\Rajendra-Tech\\GitHubRepos\\PySpark\\PySpark\\.venv\\Scripts\\python.exe\"\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ✅ Create Spark session with Delta Lake support\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"DeltaLakeExample\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f013d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [\n",
    "    Row(id=1, name=\"Alice\", age=25),\n",
    "    Row(id=2, name=\"Bob\", age=30),\n",
    "    Row(id=3, name=\"Charlie\", age=35)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"tmp/delta/people\")\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"tmp/delta/people\")\n",
    "delta_table.toDF().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb07f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ✅ Create test DataFrame\n",
    "data = [\n",
    "    Row(id=1, name=\"Alice\", age=25),\n",
    "    Row(id=2, name=\"Bob\", age=30),\n",
    "    Row(id=3, name=\"Charlie\", age=35)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# ✅ Write as Delta table\n",
    "delta_path = \"tmp/delta/people\"\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# ✅ Read Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "print(\"Initial Data:\")\n",
    "delta_table.toDF().show()\n",
    "\n",
    "# ✅ Update record\n",
    "delta_table.update(\n",
    "    condition=\"name = 'Bob'\",\n",
    "    set={\"age\": \"32\"}\n",
    ")\n",
    "print(\"After Update:\")\n",
    "delta_table.toDF().show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
