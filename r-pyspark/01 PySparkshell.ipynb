{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000021B9EAD2ED0>\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = r\"D:\\Rajendra-Tech\\GitHubRepos\\PySpark\\PySpark\\.venv\\Scripts\\python.exe\"\n",
    "\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = (\n",
    "#     SparkSession.builder\n",
    "#     .appName(\"NotebookTest\")\n",
    "#     .enableHiveSupport()         # <-- This line is needed!\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "from spark_helper import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "\n",
    "# Now use spark as usual\n",
    "print(spark)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000021B9EAD2ED0>\n"
     ]
    }
   ],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spark.conf.get(\"spark.sql.shuffle.partitions\") retrieves the current value of the Spark configuration property called spark.sql.shuffle.partitions.\n",
    "# This property controls the number of partitions to use when shuffling data for joins or aggregations in Spark SQL.\n",
    "# By default, its value is often set to 200, but you can change it to optimize performance based on your data size and cluster resources.\n",
    "# How it works:\n",
    "\n",
    "# When Spark performs operations like groupBy, join, or orderBy, it may need to redistribute (shuffle) data across the cluster.\n",
    "# The number of shuffle partitions determines how many output files (and tasks) are created during this process.\n",
    "# Setting this value too low can cause data skew and slow processing; setting it too high can create too many small tasks and overhead.\n",
    "\n",
    "# Set Config\n",
    "# spark.conf.set(\"spark.executor.memory\", \"5g\")\n",
    "\n",
    "# Get a Spark Config\n",
    "partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [(\"Scala\", 25000), (\"Spark\", 35000), (\"PHP\", 21000)])\n",
    "df.show()\n",
    "\n",
    "# Output\n",
    "#+-----+-----+\n",
    "#|   _1|   _2|\n",
    "#+-----+-----+\n",
    "#|Scala|25000|\n",
    "#|Spark|35000|\n",
    "#|  PHP|21000|\n",
    "#+-----+-----+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame  - test 2time the performance\n",
    "df = spark.createDataFrame(\n",
    "    [(\"Scala\", 25000), (\"Spark\", 35000), (\"PHP\", 21000)])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   _1|   _2|\n",
      "+-----+-----+\n",
      "|Scala|25000|\n",
      "|Spark|35000|\n",
      "|  PHP|21000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view and run a SQL query\n",
    "\n",
    "# Spark SQL\n",
    "df.createOrReplaceTempView(\"sample_table\")\n",
    "df2 = spark.sql(\"SELECT _1,_2 FROM sample_table\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://sparkbyexamples.com/pyspark/pyspark-what-is-sparksession/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install virtualenv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/D:/Rajendra-Tech/GitHubRepos/PySpark/PySpark/r-pyspark/spark-warehouse')]\n",
      "[Table(name='sample_table', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get metadata from the Catalog\n",
    "# List databases\n",
    "dbs = spark.catalog.listDatabases()\n",
    "print(dbs)\n",
    "\n",
    "# Output\n",
    "#[Database(name='default', description='default database', \n",
    "#locationUri='file:/Users/admin/.spyder-py3/spark-warehouse')]\n",
    "\n",
    "# List Tables\n",
    "tbls = spark.catalog.listTables()\n",
    "print(tbls)\n",
    "\n",
    "#Output\n",
    "#[Table(name='sample_hive_table', database='default', description=None, tableType='MANAGED', #isTemporary=False), Table(name='sample_hive_table1', database='default', description=None, #tableType='MANAGED', isTemporary=False), Table(name='sample_hive_table121', database='default', #description=None, tableType='MANAGED', isTemporary=False), Table(name='sample_table', database=None, #description=None, tableType='TEMPORARY', isTemporary=True)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
